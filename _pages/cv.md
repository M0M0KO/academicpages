---
layout: archive
title: "Curriculum Vitae"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

<div style="text-align: center; margin-bottom: 1.5em;">
  <p style="font-size: 1.1em; color: #494e52; font-style: italic; max-width: 800px; margin: 0 auto;">
    Medical Image Analysis Researcher transitioning to Embodied Intelligence
  </p>
  
  <div class="download-button">
    <a href="#" class="btn btn-primary"><i class="fas fa-download"></i> Download Full CV (PDF)</a>
  </div>
</div>

---

## Education

<div class="education">
  <div class="education-entry">
    <h3><i class="fas fa-graduation-cap" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>M.S. in Artificial Intelligence and Adaptive System</h3>
    <p><em>Sussex Artificial Intelligence Institute, Zhejiang Gongshang University</em><br>
    Hangzhou, China | 2024-Present</p>
    <ul>
      <li><strong>Research Focus:</strong> Embodied Intelligence, Robotics</li>
      <li><strong>Core Courses:</strong> Intelligence in Animals and Machines, Intelligent Systems Techniques, Image Processing, Machine Learning</li>
      <li><strong>Academic Advisor:</strong> [Professor Name]</li>
      <li><strong>Expected Graduation:</strong> June 2027</li>
    </ul>
  </div>

  <div class="education-entry">
    <h3><i class="fas fa-graduation-cap" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>B.S. in Computer Science and Technology</h3>
    <p><em>Wenzhou Business College</em><br>
    Wenzhou, China | 2019-2023</p>
    <ul>
      <li><strong>GPA:</strong> 3.41/5.0 (84.7/100)</li>
      <li><strong>Thesis:</strong> "Smoking behavior detection based on deep learning and skeletal framework" (Distinguished)</li>
      <li><strong>Relevant Coursework:</strong> Data Structures and Algorithms, Python Programming, Artificial Intelligence, Computer Vision, Data Analysis, Machine Learning</li>
      <li><strong>Thesis Advisor:</strong> [Professor Name]</li>
    </ul>
  </div>
</div>

---

## Research Experience

<div class="experience">
  <div class="experience-entry">
    <h3><i class="fas fa-flask" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Research Intern</h3>
    <p><em>Wenzhou Medical University First Affiliated Hospital - Hepato-Pancreato-biliary Surgery Laboratory</em><br>
    Wenzhou, China | Sept 2022 - Jan 2023</p>
    <ul>
      <li>Developed medical image preprocessing software for clinical applications, resulting in software copyright registration (2022SR0252378)</li>
      <li>Designed and implemented deep learning models for leukemia diagnosis based on tongue image analysis, achieving 84% diagnostic accuracy</li>
      <li>Created machine learning algorithms for exosome feature analysis in hepatocellular carcinoma research, contributing to a paper published in Frontiers in Cell and Developmental Biology</li>
      <li>Collaborated with a multidisciplinary team of 6 medical professionals to validate algorithms against clinical data</li>
      <li><strong>Technologies:</strong> PyTorch, TensorFlow, OpenCV, Medical Imaging Libraries</li>
      <li><strong>Supervisor:</strong> [Professor/Doctor Name], [Title]</li>
    </ul>
  </div>

  <div class="experience-entry">
    <h3><i class="fas fa-users" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Student Team Leader, National Innovation Training Project</h3>
    <p><em>Wenzhou Business College</em><br>
    June 2022 - June 2023</p>
    <ul>
      <li>Led a team of 4 students in research on enhancing YOLO architecture with attention mechanisms for real-time object detection</li>
      <li>Implemented and benchmarked self-attention module modifications, achieving a 7% improvement in detection accuracy while maintaining real-time performance</li>
      <li>Conducted experimental validation on multiple datasets including COCO and custom domain-specific datasets</li>
      <li>Secured 2 software copyrights and 1 patent application based on project outcomes</li>
      <li>Managed project timeline, task distribution, and documentation, ensuring project milestones were met on schedule</li>
      <li><strong>Technologies:</strong> PyTorch, YOLO, Computer Vision, CUDA</li>
      <li><strong>Advisor:</strong> [Professor Name], [Title]</li>
    </ul>
  </div>
</div>

---

## Research Interests & Expertise

<div class="research-interests">
  <div class="interest-area">
    <h3><i class="fas fa-brain" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Medical Artificial Intelligence</h3>
    <ul>
      <li>Deep learning for medical image analysis and disease diagnosis</li>
      <li>Multi-modal clinical data integration and feature extraction</li>
      <li>Computer-aided diagnosis systems for clinical applications</li>
    </ul>
  </div>
  
  <div class="interest-area">
    <h3><i class="fas fa-robot" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Embodied Intelligence</h3>
    <ul>
      <li>Robot learning and sensorimotor control</li>
      <li>Visual perception for physical interaction</li>
      <li>Reinforcement learning in robotic applications</li>
    </ul>
  </div>
  
  <div class="interest-area">
    <h3><i class="fas fa-eye" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Computer Vision</h3>
    <ul>
      <li>Attention-based object detection architectures</li>
      <li>Human pose estimation and behavior recognition</li>
      <li>Visual feature extraction and representation learning</li>
    </ul>
  </div>
  
  <div class="interest-area">
    <h3><i class="fas fa-project-diagram" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Multi-modal Learning</h3>
    <ul>
      <li>Vision-Language-Action (VLA) models</li>
      <li>Fusion of heterogeneous data sources</li>
      <li>Temporal sequence modeling for behavior analysis</li>
    </ul>
  </div>
</div>

---

## Technical Skills

<div class="skills-section">
  <div class="skills-grid">
    <div class="skill-category">
      <h3><i class="fas fa-code" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Programming Languages</h3>
      <ul>
        <li><strong>Advanced:</strong> Python</li>
        <li><strong>Intermediate:</strong> C, Java, SQL</li>
        <li><strong>Basic:</strong> C#, JavaScript, Vue</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-brain" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>AI & Machine Learning</h3>
      <ul>
        <li><strong>Frameworks:</strong> PyTorch, TensorFlow</li>
        <li><strong>Areas:</strong> Computer Vision, Deep Learning, Reinforcement Learning</li>
        <li><strong>Techniques:</strong> CNNs, Attention Mechanisms, Transfer Learning</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-tools" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Development Tools</h3>
      <ul>
        <li><strong>Version Control:</strong> Git, GitHub</li>
        <li><strong>Documentation:</strong> LaTeX, Markdown</li>
        <li><strong>Environment:</strong> Linux, Jupyter, Docker</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-chart-bar" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Data Analysis</h3>
      <ul>
        <li><strong>Libraries:</strong> NumPy, Pandas, SciPy</li>
        <li><strong>Visualization:</strong> Matplotlib, Seaborn, Plotly</li>
        <li><strong>Statistics:</strong> Hypothesis Testing, Regression Analysis</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-language" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Languages</h3>
      <ul>
        <li><strong>Chinese:</strong> Native</li>
        <li><strong>English:</strong> Professional (IELTS 6.0: L:6.5, R:6.5, W:5.5, S:6.0)</li>
        <li><strong>Certification:</strong> CET-6 467</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-certificate" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Certifications</h3>
      <ul>
        <li>DevCloud Summer Training Camp (Huawei)</li>
        <li>Python for AI Development (Shandong University)</li>
        <li>Deep Learning Fundamentals (Shandong University)</li>
      </ul>
    </div>
  </div>
</div>

---

## Publications

<div class="publications">
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Machine Learning Identifies Exosome Features Related to Hepatocellular Carcinoma</h3>
    <p><strong>Journal:</strong> <em>Frontiers in Cell and Developmental Biology</em> (September 2022)</p>
    <p><strong>Authors:</strong> Kai Zhu, Qiqi Tao, <strong>Jiatao Yan</strong>, Zhichao Lang, Xinmiao Li, Yifei Li, Congcong Fan, Zhengping Yu</p>
    <p><strong>DOI:</strong> <a href="https://doi.org/10.3389/fcell.2022.1020415" target="_blank">10.3389/fcell.2022.1020415</a></p>
    <p><strong>Impact Factor:</strong> 5.8</p>
    <p class="contribution">Contribution: Developed and implemented machine learning algorithms for exosome feature analysis. Created feature extraction pipelines and classification models that identified key patterns associated with hepatocellular carcinoma development. Participated in experimental design and data analysis.</p>
  </div>
  
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Multi-omics and Machine Learning-driven CD8+ T Cell Heterogeneity Score for Prognosis</h3>
    <p><strong>Journal:</strong> <em>Molecular Therapy Nucleic Acids</em> (December 2024)</p>
    <p><strong>Authors:</strong> Di He, Zhan Yang, Tian Zhang, Yaxian Luo, Lianjie Peng, <strong>Jiatao Yan</strong>, Tao Qiu, Jingyu Zhang, Luying Qin, Zhichao Liu, Xiaoting Zhang, Lining Lin, Mouyuan Sun</p>
    <p><strong>DOI:</strong> <a href="https://doi.org/10.1016/j.omtn.2024.102413" target="_blank">10.1016/j.omtn.2024.102413</a></p>
    <p><strong>Impact Factor:</strong> 6.4</p>
    <p class="contribution">Contribution: Implemented multiple machine learning algorithms for key gene identification in HNSCC research. Developed statistical methods for analyzing multi-omics data and contributed to computational pipeline for heterogeneity score calculation.</p>
  </div>
</div>

---
  
## Manuscripts Under Review

<div class="manuscripts">
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Deep Learning Model for Survival Prediction of Localized Upper Tract Urothelial Carcinoma Based on Multi-Phase CT Images and Clinical Data</h3>
    <p><strong>Journal:</strong> <em>BMC Cancer</em> (Submitted: June 24, 2024 - Revision Required)</p>
    <p><strong>Authors:</strong> Kai Zhu, Binwei Lin, <strong>Jiatao Yan</strong>, Honghui Zhu, Wei Chen, Xin Yao, Fengyan You, Yue Pan, Feng Wang, Peng Xia, Yeping Li, Lianguo Chen, Zhixian Yu, Shouliang Miao, Xiaomin Gao</p>
    <p class="contribution">Contribution: Designed and implemented the deep learning architecture for analyzing multi-phase CT images. Developed methods for integrating imaging features with clinical data to create a comprehensive predictive model. Participated in model validation and performance optimization.</p>
  </div>

  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Using Multiomics and Machine Learning: Insights into Improving the Outcomes of Clear Cell Renal Cell Carcinoma via SRD5A3-AS1/hsa-let-7e-5p/RRM2 Axis</h3>
    <p><strong>Journal:</strong> <em>ACS Omega</em> (Submitted: February 12, 2025 - Under Review)</p>
    <p><strong>Authors:</strong> Mouyuan Sun, Zhan Yang, Yaxian Luo, Luying Qin, Lianjie Peng, Chaoran Pan, <strong>Jiatao Yan</strong>, Tao Qiu, Yan Zhang</p>
    <p class="contribution">Contribution: Implemented machine learning algorithms for identifying regulatory components in the SRD5A3-AS1/hsa-let-7e-5p/RRM2 axis. Developed computational methods for multi-omics data integration and contributed to statistical analysis of results.</p>
  </div>
</div>

---

## Manuscripts in Preparation

<div class="manuscripts">
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>YOLOv11-LCDFS: Enhanced Smoking Detection With Low-light Enhancement</h3>
    <p><strong>Target Journal:</strong> <em>IEEE Transactions on Image Processing</em> (Expected Submission: Q3 2025)</p>
    <p><strong>Authors:</strong> <strong>Jiatao Yan</strong>, Zhuzikai Zheng, Zhengtan Yang, Hao Jiang, Peichen Wang, Fangjun Kuang, Siyang Zhang</p>
    <p class="contribution">Lead author developing a novel YOLO-based architecture with integrated low-light enhancement capability for reliable smoking behavior detection in challenging lighting conditions. Implementing attention mechanisms and custom data augmentation techniques to improve generalization.</p>
  </div>
</div>

---

## Intellectual Property

<div class="patents">
  <div class="patent-item">
    <h3><i class="fas fa-file-contract" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Patent Application</h3>
    <p><strong>Title:</strong> Smoking Behavior Recognition Camera and Determination Method</p>
    <p><strong>Application Number:</strong> 202310277784.1</p>
    <p><strong>Status:</strong> Pending (Filed March 2023)</p>
    <p><strong>Inventors:</strong> Jiatao Yan, Zhuzikai Zheng, [Other names]</p>
    <p><strong>Summary:</strong> A novel method combining pose estimation with temporal sequence analysis for real-time smoking behavior detection in public spaces.</p>
  </div>
  
  <div class="software-copyrights">
    <h3><i class="fas fa-file-code" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Software Copyrights</h3>
    <ul>
      <li>
        <strong>Medical Image Computing Software</strong> (2022SR0252378)<br>
        <em>Registered: April 2022</em><br>
        A comprehensive tool for medical image preprocessing, including format standardization, noise reduction, and feature extraction.
      </li>
      <li>
        <strong>Human Skeleton Recognition Software</strong> (2022SR1258998)<br>
        <em>Registered: October 2022</em><br>
        Software for real-time human pose estimation and skeletal framework extraction from video streams.
      </li>
      <li>
        <strong>Cigarette Recognition Software</strong> (2022SR1277520)<br>
        <em>Registered: October 2022</em><br>
        Object detection system specialized for cigarette recognition in varied environments and lighting conditions.
      </li>
      <li>
        <strong>Smoking Behavior Detection Software</strong> (2022SR1277521)<br>
        <em>Registered: October 2022</em><br>
        Integrated system combining pose estimation and object detection for accurate smoking behavior identification.
      </li>
    </ul>
  </div>
</div>

---

## Academic Achievements & Awards

<div class="achievements">
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-trophy" style="font-size: 1.2em; color: #D4AF37; margin-right: 0.5em;"></i>Stanford RNA 3D Folding Competition</h4>
      <p><strong>Bronze Medal</strong> | 87th/1093 Place (Top 8%) | Kaggle Global Competition | 2023-Ongoing</p>
      <p class="achievement-desc">Developed a deep learning approach for predicting RNA 3D structures from sequences, incorporating specialized attention mechanisms to capture long-range interactions.</p>
    </div>
  </div>

  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-chart-line" style="font-size: 1.2em; color: #4285F4; margin-right: 0.5em;"></i>Predict Podcast Listening Time Competition</h4>
      <p><strong>Ranked 154th/2565</strong> | Top 6% | Kaggle Global Competition | 2023-Ongoing</p>
      <p class="achievement-desc">Created a machine learning pipeline to predict user engagement with podcast content, combining NLP techniques with temporal user behavior analysis.</p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-trophy" style="font-size: 1.2em; color: #C0C0C0; margin-right: 0.5em;"></i>Kaggle HuBMAP + HPA Competition</h4>
      <p><strong>Top 38%</strong> | 442nd Place | Global Competition | September 2022</p>
      <p class="achievement-desc">Implemented semantic segmentation models to identify functional tissue units across different organs in microscopy images.</p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-medal" style="font-size: 1.2em; color: #CD7F32; margin-right: 0.5em;"></i>18th Challenge Cup College Student Competition</h4>
      <p><strong>Bronze Medal</strong> | Zhejiang Province Level | May 2023</p>
      <p class="achievement-desc">Developed and presented an innovative smoking behavior detection system based on computer vision and pose estimation, recognized for technical merit and practical application.</p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-award" style="font-size: 1.2em; color: #4285F4; margin-right: 0.5em;"></i>4th National "Chuanzhi Cup" IT Skills Competition</h4>
      <p><strong>Provincial Excellent Award</strong> | Zhejiang Province | December 2021</p>
      <p class="achievement-desc">Recognized for demonstrating exceptional programming and algorithm development skills in a time-constrained competitive environment.</p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-award" style="font-size: 1.2em; color: #4285F4; margin-right: 0.5em;"></i>2023 Wenzhou Computer Society Student Member Innovation and Entrepreneurship Award</h4>
      <p><strong>3rd Prize</strong> | Wenzhou | April 2024</p>
      <p class="achievement-desc">Honored for contributions to innovative applications of computer vision in public health monitoring systems.</p>
    </div>
  </div>
</div>

---

## Projects

<div class="projects">
  <div class="project-entry">
    <h3><i class="fas fa-project-diagram" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Enhanced YOLO with Attention Mechanism</h3>
    <p><strong>Role:</strong> Lead Researcher | <strong>Duration:</strong> June 2022 - June 2023</p>
    <div class="project-details">
      <p>Designed and implemented an improved YOLO architecture with self-attention modules for enhanced object detection while maintaining real-time performance capabilities.</p>
      <ul>
        <li>Integrated a lightweight attention mechanism compatible with YOLO's architecture, optimizing for inference speed</li>
        <li>Achieved a 7% improvement in mean Average Precision (mAP) over baseline YOLO on standard datasets</li>
        <li>Conducted extensive benchmarking on multiple hardware configurations to ensure real-time functionality</li>
        <li>Optimized model for deployment on edge devices with limited computational resources</li>
      </ul>
      <p><strong>Technologies:</strong> PyTorch, YOLO, Computer Vision, CUDA, TensorRT</p>
      <p><strong>Outcomes:</strong> Patent application, 2 software copyrights, technical documentation</p>
    </div>
  </div>

  <div class="project-entry">
    <h3><i class="fas fa-microscope" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Medical Image Preprocessing Pipeline</h3>
    <p><strong>Role:</strong> Developer | <strong>Duration:</strong> September 2022 - December 2022</p>
    <div class="project-details">
      <p>Developed a comprehensive preprocessing pipeline for medical images to support clinical diagnostic applications at Wenzhou Medical University First Affiliated Hospital.</p>
      <ul>
        <li>Created standardization algorithms for handling images from multiple medical imaging devices (CT, MRI, ultrasound)</li>
        <li>Implemented advanced noise reduction and artifact removal techniques specific to medical imaging challenges</li>
        <li>Designed an intuitive interface for medical professionals with limited technical background</li>
        <li>Integrated the software with existing hospital information systems for seamless workflow</li>
      </ul>
      <p><strong>Technologies:</strong> Python, OpenCV, PyDicom, NumPy, Qt for UI</p>
      <p><strong>Outcomes:</strong> Software copyright registration, deployment in clinical setting, improved diagnostic workflow efficiency by approximately 40%</p>
    </div>
  </div>
  
  <div class="project-entry">
    <h3><i class="fas fa-smoking" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Smoking Behavior Detection System</h3>
    <p><strong>Role:</strong> Lead Developer | <strong>Duration:</strong> January 2022 - May 2023</p>
    <div class="project-details">
      <p>Undergraduate thesis project focused on automatically detecting smoking behavior using pose estimation and deep learning techniques.</p>
      <ul>
        <li>Developed a novel approach combining skeletal framework analysis with temporal modeling to identify characteristic smoking gestures</li>
        <li>Created custom datasets with over 10,000 annotated frames for model training and validation</li>
        <li>Implemented temporal sequence modeling to differentiate smoking from similar hand-to-mouth actions</li>
        <li>Achieved 89% detection accuracy in controlled environments and 76% in uncontrolled real-world scenarios</li>
      </ul>
      <p><strong>Technologies:</strong> Deep Learning, Human Pose Estimation (OpenPose), Action Recognition, PyTorch, OpenCV</p>
      <p><strong>Outcomes:</strong> Patent application, 2 software copyrights, thesis received distinguished evaluation</p>
    </div>
  </div>
</div>

---

## Professional Development

<div class="professional-development">
  <div class="current-learning">
    <h3><i class="fas fa-brain" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Current Research Focus</h3>
    <p>Self-directed advanced study in embodied intelligence and robotics, focusing on the following areas:</p>
    <ul>
      <li><strong>Foundations of Robotics:</strong> Kinematics, dynamics, motion planning, and control systems</li>
      <li><strong>Reinforcement Learning:</strong> Policy optimization, multi-agent RL, and sim-to-real transfer</li>
      <li><strong>Vision-Language-Action Models:</strong> Multimodal foundation models for robotic control</li>
      <li><strong>Sensorimotor Learning:</strong> Integrating perception and action in embodied agents</li>
      <li><strong>Simulation Environments:</strong> MuJoCo, Isaac Gym, and Habitat for embodied AI research</li>
    </ul>
  </div>
  
  <div class="key-resources">
    <h3><i class="fas fa-book" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Advanced Study Resources</h3>
    <ul>
      <li>
        <strong>Technical Textbooks:</strong>
        <ul>
          <li>"Modern Robotics: Mechanics, Planning, and Control" by Kevin Lynch and Frank Park</li>
          <li>"Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto</li>
          <li>"Probabilistic Robotics" by Sebastian Thrun, Wolfram Burgard, and Dieter Fox</li>
        </ul>
      </li>
      <li>
        <strong>Graduate-Level Courses:</strong>
        <ul>
          <li>Stanford CS231n: Convolutional Neural Networks for Visual Recognition</li>
          <li>UC Berkeley CS285: Deep Reinforcement Learning</li>
          <li>MIT 6.S094: Deep Learning for Self-Driving Cars</li>
        </ul>
      </li>
      <li>
        <strong>Research Literature:</strong>
        <ul>
          <li>Regular review of recent papers from ICRA, CoRL, NeurIPS, CVPR, and ICLR conferences</li>
          <li>Following research from leading labs: Berkeley AI Research, Stanford AI Lab, FAIR, Google DeepMind</li>
        </ul>
      </li>
    </ul>
  </div>
</div>

---

## References

<div class="references">
  <p>Professional and academic references available upon request.</p>
</div>

<style>
  /* Global Styles */
  h2 {
    margin-top: 2em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    border-bottom: 2px solid #f0f0f0;
    color: #333;
    font-weight: 600;
  }
  
  hr {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.15), rgba(0, 0, 0, 0));
    margin: 2em 0;
  }
  
  .download-button {
    text-align: center;
    margin: 1.5em 0;
  }
  
  .btn-primary {
    display: inline-block;
    padding: 0.5em 1em;
    background-color: #2a76dd;
    color: white;
    text-decoration: none;
    border-radius: 4px;
    font-weight: bold;
    transition: background-color 0.3s ease;
  }
  
  .btn-primary:hover {
    background-color: #1a66cd;
    text-decoration: none;
  }

  /* Section Styles */
  .education-entry,
  .experience-entry,
  .project-entry,
  .paper {
    margin-bottom: 2em;
    padding: 1.5em;
    background-color: #f8f9fa;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
  }
  
  .education-entry:hover,
  .experience-entry:hover,
  .project-entry:hover,
  .paper:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .education-entry h3,
  .experience-entry h3,
  .project-entry h3,
  .paper h3,
  .interest-area h3 {
    margin-bottom: 0.8em;
    color: #494e52;
    font-weight: 600;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.5em;
  }
  
  .education-entry p,
  .experience-entry p,
  .project-entry p,
  .paper p {
    margin: 0.5em 0;
    line-height: 1.5;
  }
  
  .education-entry ul,
  .experience-entry ul,
  .project-details ul,
  .interest-area ul,
  .current-learning ul,
  .key-resources ul,
  .software-copyrights ul {
    padding-left: 1.5em;
  }
  
  .education-entry li,
  .experience-entry li,
  .project-details li,
  .interest-area li,
  .current-learning li,
  .key-resources li,
  .software-copyrights li {
    margin-bottom: 0.5em;
    line-height: 1.4;
  }
  
  .project-details {
    margin-top: 1em;
  }
  
  /* Publication Styles */
  .contribution {
    font-style: italic;
    color: #666;
    border-left: 3px solid #ddd;
    padding-left: 10px;
    margin-top: 0.8em;
    background-color: #f9f9f9;
    padding: 0.5em 1em;
    border-radius: 0 4px 4px 0;
  }
  
  /* Research Interests */
  .research-interests {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
    grid-gap: 1.5em;
  }
  
  .interest-area {
    padding: 1.5em;
    background-color: #f8f9fa;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
  }
  
  .interest-area:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  /* Skills Grid */
  .skills-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    grid-gap: 1.5em;
  }
  
  .skill-category {
    background-color: #f8f9fa;
    padding: 1.5em;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
  }
  
  .skill-category:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .skill-category h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: #494e52;
    font-weight: 600;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.5em;
  }
  
  /* Achievements */
  .achievements {
    display: grid;
    grid-template-columns: 1fr;
    grid-gap: 1.5em;
  }
  
  .achievement-item {
    background-color: #f8f9fa;
    border-radius: 8px;
    overflow: hidden;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    padding: 1.5em;
    transition: all 0.2s ease-in-out;
  }
  
  .achievement-item:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .achievement-content h4 {
    margin: 0 0 0.5em 0;
    color: #494e52;
    display: flex;
    align-items: center;
    font-weight: 600;
  }
  
  .achievement-content p {
    margin: 0.5em 0;
    font-size: 0.95em;
    line-height: 1.5;
    color: #333;
  }
  
  .achievement-desc {
    font-size: 0.9em !important;
    color: #666 !important;
    margin-top: 0.5em !important;
  }
  
  /* Patents and Software */
  .patents {
    margin-bottom: 2em;
  }
  
  .patent-item {
    padding: 1.5em;
    border-radius: 8px;
    background-color: #f8f9fa;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
    margin-bottom: 1.5em;
  }
  
  .patent-item:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .patent-item h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: #494e52;
    font-weight: 600;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.5em;
  }
  
  .patent-item p {
    margin: 0.3em 0;
    line-height: 1.4;
  }
  
  .software-copyrights {
    padding: 1.5em;
    border-radius: 8px;
    background-color: #f8f9fa;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  }
  
  .software-copyrights h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: #494e52;
    font-weight: 600;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.5em;
  }
  
  .software-copyrights li {
    margin-bottom: 1em;
  }
  
  /* Professional Development */
  .professional-development > div {
    padding: 1.5em;
    background-color: #f8f9fa;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    margin-bottom: 1.5em;
    transition: all 0.2s ease-in-out;
  }
  
  .professional-development > div:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .professional-development h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: #494e52;
    font-weight: 600;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.5em;
  }
  
  /* References */
  .references {
    padding: 1.5em;
    background-color: #f8f9fa;
    border-radius: 8px;
    text-align: center;
    font-style: italic;
  }
  
  /* Icons */
  .fa, .fas, .far, .fab {
    color: #494e52;
  }
  
  .achievement-content .fas {
    color: inherit;
  }
  
  /* Responsive Adjustments */
  @media (max-width: 768px) {
    .research-interests,
    .skills-grid {
      grid-template-columns: 1fr;
    }
  }
</style>
