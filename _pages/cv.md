---
layout: archive
title: "Curriculum Vitae"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

<div style="text-align: center; margin-bottom: 1.5em;">
  <p style="font-size: 1.1em; color: #494e52; font-style: italic; max-width: 800px; margin: 0 auto;">
    Computer Vision and Medical AI Postgraduate Student | Exploring Embodied Intelligence
  </p>
  
  <div class="download-button">
    <a href="{{ base_path }}/files/CV.pdf" class="btn btn-primary"><i class="fas fa-download"></i> Download Full CV (PDF)</a>
  </div>
</div>

---

## Education

<div class="education">
  <div class="education-entry">
    <h3><i class="fas fa-graduation-cap" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>M.S. in Artificial Intelligence and Adaptive System</h3>
    <p><em>Sussex Artificial Intelligence Institute, Zhejiang Gongshang University</em><br>
    Hangzhou, China | 2024-Present</p>
    <ul>
      <li><strong>Research Focus:</strong> Embodied Intelligence, Robotics</li>
      <li><strong>Core Courses:</strong> Intelligence in Animals and Machines, Intelligent Systems Techniques, Image Processing, Machine Learning</li>
      <li><strong>supervisor:</strong> Assistant Professor Peter Wijeratne (University of Sussex) and Professor Xie Mande (Zhejiang Gongshang University)</li>
      <li><strong>Expected Graduation:</strong> June 2027</li>
    </ul>
  </div>

  <div class="education-entry">
    <h3><i class="fas fa-graduation-cap" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>B.S. in Computer Science and Technology</h3>
    <p><em>Wenzhou Business College</em><br>
    Wenzhou, China | 2019-2023</p>
    <ul>
      <li><strong>GPA:</strong> 3.41/5.0 (84.7/100)</li>
      <li><strong>Thesis:</strong> "Smoking behavior detection based on deep learning and skeletal framework"</li>
      <li><strong>Relevant Coursework:</strong> Data Structures and Algorithms, Python Programming, Artificial Intelligence, Computer Vision, Data Analysis, Machine Learning</li>
      <li><strong>Thesis Advisor:</strong> Prof. Fangjun Kuang</li>
    </ul>
  </div>
</div>

---

## Research Experience

<div class="experience">
  <div class="experience-entry">
    <h3><i class="fas fa-flask" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Research Intern</h3>
    <p><em>Wenzhou Medical University First Affiliated Hospital - Hepato-Pancreato-biliary Surgery Laboratory</em><br>
    Wenzhou, China | Sept 2022 - Jan 2023</p>
    <ul>
      <li>Assisted in developing medical image preprocessing software for clinical applications, resulting in software copyright registration (2022SR0252378)</li>
      <li>Contributed to deep learning models for leukemia diagnosis based on tongue image analysis</li>
      <li>Helped create machine learning algorithms for exosome feature analysis in hepatocellular carcinoma research, contributing to a paper published in Frontiers in Cell and Developmental Biology</li>
      <li><strong>Technologies:</strong> PyTorch, TensorFlow, OpenCV</li>
    </ul>
  </div>

  <div class="experience-entry">
    <h3><i class="fas fa-users" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Student Team Leader, National Innovation Training Project</h3>
    <p><em>Wenzhou Business College</em><br>
    June 2022 - June 2023</p>
    <ul>
      <li>Coordinated a team of 4 students in research on enhancing YOLO architecture with attention mechanisms for real-time object detection</li>
      <li>Implemented and benchmarked self-attention module modifications, achieving  improvement in detection accuracy while maintaining real-time performance</li>
      <li>Conducted experimental validation on multiple datasets including public datasets and custom datasets</li>
      <li>Contributed to securing 2 software copyrights and 1 patent application based on project outcomes</li>
      <li>Managed project timeline, task distribution, and documentation, ensuring project milestones were met on schedule</li>
      <li><strong>Technologies:</strong> PyTorch, YOLO, Computer Vision, OpenCV</li>
      <li><strong>Advisor:</strong> Prof. Siyang Zhang</li>
    </ul>
  </div>
</div>

---

## Research Interests

<div class="research-interests">
  <div class="interest-area">
    <h3><i class="fas fa-brain" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Medical Artificial Intelligence</h3>
    <ul>
      <li>Learning about deep learning for medical image analysis and disease diagnosis</li>
      <li>Understanding multi-modal clinical data integration and feature extraction</li>
      <li>Exploring computer-aided diagnosis systems for clinical applications</li>
    </ul>
  </div>
  
  <div class="interest-area">
    <h3><i class="fas fa-robot" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Embodied Intelligence</h3>
    <ul>
      <li>Studying how robots develop intelligence through physical interaction with the world</li>
      <li>Exploring the connection between perception and action in embodied agents</li>
      <li>Learning about reinforcement learning in robotic applications</li>
    </ul>
  </div>
  
  <div class="interest-area">
    <h3><i class="fas fa-eye" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Computer Vision</h3>
    <ul>
      <li>Attention-based object detection architectures</li>
      <li>Human pose estimation and behavior recognition</li>
      <li>Visual feature extraction for real-world applications</li>
    </ul>
  </div>
  
  <div class="interest-area">
    <h3><i class="fas fa-users" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Multi-Agent Systems</h3>
    <ul>
      <li>Understanding how multiple agents can self-organize to adapt specialized roles</li>
      <li>Studying emergent behaviors and collective intelligence in agent communities</li>
      <li>Exploring approaches for effective division of labor in collaborative AI systems</li>
    </ul>
  </div>
</div>

---

## Technical Skills

<div class="skills-section">
  <div class="skills-grid">
    <div class="skill-category">
      <h3><i class="fas fa-code" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Programming Languages</h3>
      <ul>
        <li>Python</li>
        <li>C, Java, SQL</li>
        <li>C#, JavaScript, Vue</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-brain" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>AI & Machine Learning</h3>
      <ul>
        <li><strong>Frameworks:</strong> PyTorch, TensorFlow</li>
        <li><strong>Areas:</strong> Computer Vision, Deep Learning, Reinforcement Learning</li>
        <li><strong>Techniques:</strong> CNNs, Attention Mechanisms, Transfer Learning</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-tools" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Development Tools</h3>
      <ul>
        <li><strong>Version Control:</strong> Git, GitHub</li>
        <li><strong>Documentation:</strong> LaTeX, Markdown</li>
        <li><strong>Environment:</strong> Linux, Jupyter, Docker</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-chart-bar" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Data Analysis</h3>
      <ul>
        <li><strong>Libraries:</strong> NumPy, Pandas, SciPy</li>
        <li><strong>Visualization:</strong> Matplotlib, Seaborn, Plotly</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-language" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Languages</h3>
      <ul>
        <li><strong>Chinese:</strong> Native</li>
        <li><strong>English:</strong> Professional (IELTS 6.0: L:6.5, R:6.5, W:5.5, S:6.0)</li>
        <li><strong>Certification:</strong> CET-6 467</li>
      </ul>
    </div>
    
    <div class="skill-category">
      <h3><i class="fas fa-certificate" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Certifications</h3>
      <ul>
        <li>DevCloud Summer Training Camp (Huawei)</li>
        <li>Python for AI Development (Shandong University)</li>
        <li>Deep Learning Fundamentals (Shandong University)</li>
      </ul>
    </div>
  </div>
</div>

---

## Publications

<div class="publications">
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Machine Learning Identifies Exosome Features Related to Hepatocellular Carcinoma</h3>
    <p><strong>Journal:</strong> <em>Frontiers in Cell and Developmental Biology</em> (September 2022)</p>
    <p><strong>Authors:</strong> Kai Zhu, Qiqi Tao, <strong>Jiatao Yan</strong>, Zhichao Lang, Xinmiao Li, Yifei Li, Congcong Fan, Zhengping Yu</p>
    <p><strong>DOI:</strong> <a href="https://doi.org/10.3389/fcell.2022.1020415" target="_blank">10.3389/fcell.2022.1020415</a></p>
    <p><strong>Impact Factor:</strong> 5.8</p>
    <p class="contribution">Contribution: Assisted in developing machine learning algorithms for exosome feature analysis and classification in hepatocellular carcinoma research</p>
  </div>
  
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Multi-omics and Machine Learning-driven CD8+ T Cell Heterogeneity Score for Prognosis</h3>
    <p><strong>Journal:</strong> <em>Molecular Therapy Nucleic Acids</em> (December 2024)</p>
    <p><strong>Authors:</strong> Di He, Zhan Yang, Tian Zhang, Yaxian Luo, Lianjie Peng, <strong>Jiatao Yan</strong>, Tao Qiu, Jingyu Zhang, Luying Qin, Zhichao Liu, Xiaoting Zhang, Lining Lin, Mouyuan Sun</p>
    <p><strong>DOI:</strong> <a href="https://doi.org/10.1016/j.omtn.2024.102413" target="_blank">10.1016/j.omtn.2024.102413</a></p>
    <p><strong>Impact Factor:</strong> 6.4</p>
    <p class="contribution">Contribution: Helped implement various machine learning algorithms for key gene identification in HNSCC research.</p>
  </div>
</div>

---

## Manuscripts Under Review

<div class="manuscripts">
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Using Multiomics and Machine Learning: Insights into Improving the Outcomes of Clear Cell Renal Cell Carcinoma via SRD5A3-AS1/hsa-let-7e-5p/RRM2 Axis</h3>
    <p><strong>Journal:</strong> <em>ACS Omega</em> (Submitted: February 12, 2025 - Under Review)</p>
    <p><strong>Authors:</strong> Mouyuan Sun, Zhan Yang, Yaxian Luo, Luying Qin, Lianjie Peng, Chaoran Pan, <strong>Jiatao Yan</strong>, Tao Qiu, Yan Zhang</p>
    <p class="contribution">Contribution: Implemented machine learning algorithms for identifying regulatory components in the SRD5A3-AS1/hsa-let-7e-5p/RRM2 axis.</p>
  </div>
</div>

---

## Manuscripts in Preparation

<div class="manuscripts">
  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Deep Learning Model for Survival Prediction of Localized Upper Tract Urothelial Carcinoma Based on Multi-Phase CT Images and Clinical Data</h3>
    <p><strong>In Revision</strong></p>
    <p><strong>Authors:</strong> Kai Zhu, Binwei Lin, <strong>Jiatao Yan</strong>, Honghui Zhu, Wei Chen, Xin Yao, Fengyan You, Yue Pan, Feng Wang, Peng Xia, Yeping Li, Lianguo Chen, Zhixian Yu, Shouliang Miao, Xiaomin Gao</p>
    <p class="contribution">Contribution: Designed and implemented the deep learning architecture for analyzing multi-phase CT images. Developed methods for integrating imaging features with clinical data to create a comprehensive predictive model. Participated in model validation and performance optimization.</p>
  </div>

  <div class="paper">
    <h3><i class="fas fa-file-alt" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>YOLOv11-LCDFS: Enhanced Smoking Detection With Low-light Enhancement</h3>
    <p><strong>In Revision</strong></p>
    <p><strong>Authors:</strong> <strong>Jiatao Yan</strong>, Zhuzikai Zheng, Zhengtan Yang, Hao Jiang, Peichen Wang, Fangjun Kuang, Siyang Zhang</p>
    <p class="contribution">First author working on a YOLO-based architecture with integrated low-light enhancement capabilities, specialized loss functions, attention mechanisms, and optimized upsampling techniques for improved detection in challenging lighting conditions.</p>
  </div>
</div>

---

## Intellectual Property

<div class="patents">
  <div class="patent-item">
    <h3><i class="fas fa-file-contract" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Patent Application</h3>
    <p><strong>Title:</strong> Smoking Behavior Recognition Camera and Determination Method</p>
    <p><strong>Application Number:</strong> 202310277784.1</p>
    <p><strong>Status:</strong> Application withdrawn by patent office</p>
    <p><strong>Inventors:</strong> Jiatao Yan, Siyang Zhang, Fangjun Kuang, Peicheng Wang, Zhuzikai Zheng, Hao Jiang, Zhengtan Yang, Hanwen Bao, Chunqiu Xia</p>
    <p><strong>Summary:</strong> A method combining pose estimation for real-time smoking behavior detection in public spaces.</p>
  </div>
  
  <div class="software-copyrights">
    <h3><i class="fas fa-file-code" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Software Copyrights</h3>
    <ul>
      <li>
        <strong>Medical Image Computing Software</strong> (2022SR0252378)<br>
        <em>Registered: April 2022</em><br>
        A comprehensive tool for medical image preprocessing, including format standardization, noise reduction, and feature extraction.
      </li>
      <li>
        <strong>Human Skeleton Recognition Software</strong> (2022SR1258998)<br>
        <em>Registered: October 2022</em><br>
        Software for real-time human pose estimation and skeletal framework extraction from video streams.
      </li>
      <li>
        <strong>Cigarette Recognition Software</strong> (2022SR1277520)<br>
        <em>Registered: October 2022</em><br>
        Object detection system specialized for cigarette recognition in varied environments and lighting conditions.
      </li>
      <li>
        <strong>Smoking Behavior Detection Software</strong> (2022SR1277521)<br>
        <em>Registered: October 2022</em><br>
        Integrated system combining pose estimation and object detection for accurate smoking behavior identification.
      </li>
    </ul>
  </div>
</div>

---

## Academic Achievements & Awards

<div class="achievements">
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-trophy" style="font-size: 1.2em; color: #D4AF37; margin-right: 0.5em;"></i>Stanford RNA 3D Folding Competition</h4>
      <p><strong>Bronze Medal</strong> | 87th/1093 Place (Top 8%) | Kaggle Global Competition | Ongoing</p>
      <p class="achievement-desc"></p>
    </div>
  </div>

  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-chart-line" style="font-size: 1.2em; color: #4285F4; margin-right: 0.5em;"></i>Predict Podcast Listening Time Competition</h4>
      <p><strong>Ranked 154th/2565</strong> | Top 6% | Kaggle Global Competition | Ongoing</p>
      <p class="achievement-desc"></p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-trophy" style="font-size: 1.2em; color: #C0C0C0; margin-right: 0.5em;"></i>HuBMAP + HPA Competition</h4>
      <p><strong>442nd Place</strong> | Top 38% | Kaggle Global Competition | September 2022</p>
      <p class="achievement-desc"></p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-medal" style="font-size: 1.2em; color: #CD7F32; margin-right: 0.5em;"></i>18th Challenge Cup College Student Competition</h4>
      <p><strong>Bronze Medal</strong> | Zhejiang Province Level | May 2023</p>
      <p class="achievement-desc"></p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-award" style="font-size: 1.2em; color: #4285F4; margin-right: 0.5em;"></i>4th National "Chuanzhi Cup" IT Skills Competition</h4>
      <p><strong>Provincial Excellent Award</strong> | Zhejiang Province | December 2021</p>
      <p class="achievement-desc"></p>
    </div>
  </div>
  
  <div class="achievement-item">
    <div class="achievement-content">
      <h4><i class="fas fa-award" style="font-size: 1.2em; color: #4285F4; margin-right: 0.5em;"></i>2023 Wenzhou Computer Society Student Member Innovation and Entrepreneurship Award</h4>
      <p><strong>3rd Prize</strong> | Wenzhou | April 2024</p>
      <p class="achievement-desc"></p>
    </div>
  </div>
</div>

---

## Projects

<div class="projects">
  <div class="project-entry">
    <h3><i class="fas fa-project-diagram" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>YOLOv11-LCDFS: Enhanced Smoking Detection With Low-light Enhancement</h3>
    <div class="project-details">
      <p>Extension of smoking detection research focusing on improved object detection in challenging lighting conditions. Developing a novel YOLO-based architecture with specialized components addressing the unique challenges of low-light environments.</p>
      <ul>
        <li>Designing custom loss functions specifically optimized for low-light object detection scenarios</li>
        <li>Implementing attention mechanisms to focus on key visual features in varying illumination conditions</li>
        <li>Optimizing upsampling techniques to preserve fine details in dark environments</li>
        <li>Integrating lightweight low-light enhancement module directly into the detection pipeline</li>
        <li>Preparing manuscript for submission to IEEE Transactions on Image Processing</li>
      </ul>
      <p><strong>Technologies:</strong> PyTorch, YOLO, Computer Vision, CUDA, Attention Mechanisms</p>
      <p><strong>Related Publications:</strong> Manuscript in preparation (First author)</p>
    </div>
  </div>

  <div class="project-entry">
    <h3><i class="fas fa-microscope" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Multi-modal Medical Image Analysis for Cancer Research</h3>
    <div class="project-details">
      <p>Developed medical image analysis systems for cancer research at Wenzhou Medical University First Affiliated Hospital.</p>
      <ul>
        <li>Designed medical image segmentation algorithms for hepatocellular carcinoma and renal cell carcinoma</li>
        <li>Designed methods for integrating clinical tabular data with imaging features for comprehensive analysis</li>
        <li>Implemented multi-modal fusion techniques for combining different CT scan phases</li>
        <li>Developed 3D volumetric segmentation approaches for comprehensive anatomical analysis</li>
      </ul>
      <p><strong>Technologies:</strong> Python, Deep Learning, 3D Segmentation, Multi-modal Fusion, PyDicom, NumPy</p>
      <p><strong>Related Publications:</strong> Frontiers in Cell and Developmental Biology (2022), Molecular Therapy Nucleic Acids (2024)</p>
    </div>
  </div>
  
  <div class="project-entry">
    <h3><i class="fas fa-disease" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Disease Detection Using Deep Learning</h3>
    <div class="project-details">
      <p>Developed deep learning models for automated disease detection and classification from medical imaging data, with a focus on diabetic foot ulcer detection.</p>
      <ul>
        <li>Implemented YOLO-based object detection models for precise localization of diabetic foot ulcers in clinical images</li>
        <li>Worked with multiple diabetic foot datasets to train and validate the model across diverse patient populations</li>
        <li>Applied transfer learning techniques using pre-trained models (ResNet, DenseNet) adapted to medical imaging</li>
        <li>Utilized data augmentation strategies to address limited dataset size and improve model generalization</li>
        <li>Implemented visualization techniques for model interpretability (Grad-CAM, attention maps) to assist clinicians</li>
        <li>Achieved accurate detection of ulcers at various stages of development, aiding in early intervention</li>
      </ul>
      <p><strong>Technologies:</strong> PyTorch, YOLO, OpenCV, Transfer Learning</p>
    </div>
  </div>
  
  <div class="project-entry">
    <h3><i class="fas fa-comment" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Twitter Sentiment Analysis</h3>
    <div class="project-details">
      <p>Built a sentiment analysis system for Twitter comments using natural language processing techniques.</p>
      <ul>
        <li>Implemented text preprocessing pipeline for Twitter-specific content (hashtags, mentions, emojis)</li>
        <li>Explored and compared various feature extraction methods</li>
        <li>Developed and evaluated multiple classification models (LSTM, traditional ML approaches)</li>
      </ul>
      <p><strong>Technologies:</strong> Python, NLTK, spaCy, HuggingFace Transformers, Matplotlib</p>
    </div>
  </div>
  
  <div class="project-entry">
    <h3><i class="fas fa-envelope" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Spam Email Detection</h3>
    <div class="project-details">
      <p>Developed a machine learning system to classify emails as spam or legitimate based on content analysis.</p>
      <ul>
        <li>Implemented text preprocessing and feature extraction techniques for email content</li>
        <li>Trained and evaluated multiple classification models (Naive Bayes, SVM, Random Forest)</li>
        <li>Conducted feature importance analysis to identify key indicators of spam emails</li>
      </ul>
      <p><strong>Technologies:</strong> Python, scikit-learn, Pandas, Natural Language Processing</p>
    </div>
  </div>

  <div class="project-entry">
    <h3><i class="fas fa-smoking" style="font-size: 1.2em; color: #494e52; margin-right: 0.5em;"></i>Smoking Behavior Detection System</h3>
    <div class="project-details">
      <p>Undergraduate thesis project combining YOLO object detection with MediaPipe skeletal tracking to accurately identify and classify smoking gestures in video streams.</p>
      <ul>
        <li>Utilized YOLO object detection to identify cigarettes and related objects in video footage</li>
        <li>Implemented MediaPipe for real-time skeletal tracking and pose estimation</li>
        <li>Designed algorithms to recognize characteristic smoking hand-to-mouth gesture patterns</li>
        <li>Created custom datasets with over 10,000 annotated frames for model training and validation</li>
        <li>Integrated temporal sequence modeling to differentiate smoking from similar hand-to-mouth actions</li>
      </ul>
      <p><strong>Technologies:</strong> YOLO, MediaPipe, Pose Estimation, Action Recognition, PyTorch, OpenCV</p>
      <p><strong>Outcomes:</strong> Patent application (202310277784.1), 2 software copyrights, thesis received distinguished evaluation</p>
    </div>
  </div>

</div>

---

## Professional Development

<div class="professional-development">
  <div class="current-learning">
    <h3><i class="fas fa-brain" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Current Learning Focus</h3>
    <p>Self-directed study in embodied intelligence and robotics, focusing on the following areas:</p>
    <ul>
      <li><strong>Core Algorithms for Embodied AI:</strong> Studying reinforcement learning, imitation learning, model predictive control, and diffusion models for robotics</li>
      <li><strong>Vision-Language-Action Models:</strong> Learning about foundation models that integrate perception, language understanding, and action generation for robotic control</li>
      <li><strong>Robot Learning:</strong> Exploring techniques for manipulation and navigation in diverse environments with a focus on sim-to-real transfer</li>
      <li><strong>Multi-Agent Systems:</strong> Investigating coordination mechanisms and emergent behaviors in multi-agent reinforcement learning scenarios</li>
      <li><strong>Simulation Environments:</strong> Developing expertise with MuJoCo, Isaac Gym, or Habitat for embodied AI research and development</li>
    </ul>
  </div>
  
  <div class="key-resources">
    <h3><i class="fas fa-book" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Key Learning Resources</h3>
    <ul>
      <li>
        <strong>GitHub:</strong>
        <ul>
          <li><strong>Embodied-AI-Guide</strong> (github.com/tianxingchen/Embodied-AI-Guide): A comprehensive guide to embodied intelligence research with 4.6k+ stars. This repository covers:
            <ul>
              <li>Core algorithms including reinforcement learning, imitation learning, model predictive control, and diffusion models</li>
              <li>Vision-Language-Action (VLA) models for robotic control</li>
              <li>Hardware platforms and simulator environments like MuJoCo, Isaac Gym, and Habitat</li>
              <li>Computer vision and 3D perception techniques for embodied agents</li>
              <li>Robot learning approaches for manipulation and navigation tasks</li>
            </ul>
          </li>
          <li><strong>Embodied-AI-Paper-List</strong> (github.com/Lumina-EAI/Embodied-AI-Paper-List): Curated collection of important research papers classified by topic</li>
          <li><strong>Awesome-Embodied-AI-Job</strong> (github.com/StarCycle/Awesome-Embodied-AI-Job): Resource tracking research opportunities in embodied intelligence</li>
        </ul>
      </li>
      <li>
        <strong>Research Literature:</strong>
        <ul>
          <li>Following recent papers from ICRA, CoRL, NeurIPS, CVPR, and ICLR conferences focused on embodied AI</li>
          <li>Studying foundation models for robot learning, especially works on vision-language-action models</li>
          <li>Tracking developments in LLM-based robotic planning and control</li>
          <li>Exploring sim-to-real transfer techniques for robotic manipulation</li>
        </ul>
      </li>
    </ul>
  </div>
  
  <div class="future-directions">
    <h3><i class="fas fa-compass" style="font-size: 1.2em; margin-right: 0.5em; color: #494e52;"></i>Research Questions I'm Interested In</h3>
    <ul>
      <li>How might robots develop intelligence through interaction with their environment?</li>
      <li>What mechanisms enable multiple agents to self-organize and develop specialized roles?</li>
      <li>How can AI systems automatically decompose complex tasks and utilize specialized tools?</li>
      <li>What role does physical embodiment play in developing robust and generalizable intelligence?</li>
    </ul>
  </div>
</div>

---

## References

<div class="references">
  <p>Professional and academic references available upon request.</p>
</div>

<style>
  /* Global Styles */
  h2 {
    margin-top: 2em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    border-bottom: 2px solid var(--global-border-color);
    color: var(--global-text-color);
    font-weight: 600;
  }
  
  hr {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.15), rgba(0, 0, 0, 0));
    margin: 2em 0;
  }
  
  .download-button {
    text-align: center;
    margin: 1.5em 0;
  }
  
  .btn-primary {
    display: inline-block;
    padding: 0.5em 1em;
    background-color: #2a76dd;
    color: white;
    text-decoration: none;
    border-radius: 4px;
    font-weight: bold;
    transition: background-color 0.3s ease;
  }
  
  .btn-primary:hover {
    background-color: #1a66cd;
    text-decoration: none;
  }

  /* Section Styles */
  .education-entry,
  .experience-entry,
  .project-entry,
  .paper {
    margin-bottom: 2em;
    padding: 1.5em;
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
  }
  
  .education-entry:hover,
  .experience-entry:hover,
  .project-entry:hover,
  .paper:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .education-entry h3,
  .experience-entry h3,
  .project-entry h3,
  .paper h3,
  .interest-area h3 {
    margin-bottom: 0.8em;
    color: var(--global-text-color);
    font-weight: 600;
    border-bottom: 1px solid var(--global-border-color);
    padding-bottom: 0.5em;
  }
  
  .education-entry p,
  .experience-entry p,
  .project-entry p,
  .paper p {
    margin: 0.5em 0;
    line-height: 1.5;
    color: var(--global-text-color);
  }
  
  .education-entry ul,
  .experience-entry ul,
  .project-details ul,
  .interest-area ul,
  .current-learning ul,
  .key-resources ul,
  .software-copyrights ul {
    padding-left: 1.5em;
    color: var(--global-text-color);
  }
  
  .education-entry li,
  .experience-entry li,
  .project-details li,
  .interest-area li,
  .current-learning li,
  .key-resources li,
  .software-copyrights li {
    margin-bottom: 0.5em;
    line-height: 1.4;
  }
  
  .project-details {
    margin-top: 1em;
  }
  
  /* Publication Styles */
  .contribution {
    font-style: italic;
    color: var(--global-text-color-light);
    border-left: 3px solid var(--global-border-color);
    padding-left: 10px;
    margin-top: 0.8em;
    background-color: var(--global-bg-color);
    padding: 0.5em 1em;
    border-radius: 0 4px 4px 0;
  }
  
  /* Research Interests */
  .research-interests {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(400px, 1fr));
    grid-gap: 1.5em;
  }
  
  .interest-area {
    padding: 1.5em;
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
  }
  
  .interest-area:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  /* Skills Grid */
  .skills-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
    grid-gap: 1.5em;
  }
  
  .skill-category {
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    padding: 1.5em;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
  }
  
  .skill-category:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .skill-category h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: var(--global-text-color);
    font-weight: 600;
    border-bottom: 1px solid var(--global-border-color);
    padding-bottom: 0.5em;
  }
  
  /* Achievements */
  .achievements {
    display: grid;
    grid-template-columns: 1fr;
    grid-gap: 1.5em;
  }
  
  .achievement-item {
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    border-radius: 8px;
    overflow: hidden;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    padding: 1.5em;
    transition: all 0.2s ease-in-out;
  }
  
  .achievement-item:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .achievement-content h4 {
    margin: 0 0 0.5em 0;
    color: var(--global-text-color);
    display: flex;
    align-items: center;
    font-weight: 600;
  }
  
  .achievement-content p {
    margin: 0.5em 0;
    font-size: 0.95em;
    line-height: 1.5;
    color: var(--global-text-color);
  }
  
  .achievement-desc {
    font-size: 0.9em !important;
    color: var(--global-text-color-light) !important;
    margin-top: 0.5em !important;
  }
  
  /* Patents and Software */
  .patents {
    margin-bottom: 2em;
  }
  
  .patent-item {
    padding: 1.5em;
    border-radius: 8px;
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    transition: all 0.2s ease-in-out;
    margin-bottom: 1.5em;
  }
  
  .patent-item:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .patent-item h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: var(--global-text-color);
    font-weight: 600;
    border-bottom: 1px solid var(--global-border-color);
    padding-bottom: 0.5em;
  }
  
  .patent-item p {
    margin: 0.3em 0;
    line-height: 1.4;
    color: var(--global-text-color);
  }
  
  .software-copyrights {
    padding: 1.5em;
    border-radius: 8px;
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  }
  
  .software-copyrights h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: var(--global-text-color);
    font-weight: 600;
    border-bottom: 1px solid var(--global-border-color);
    padding-bottom: 0.5em;
  }
  
  .software-copyrights li {
    margin-bottom: 1em;
    color: var(--global-text-color);
  }
  
  /* Professional Development */
  .professional-development > div {
    padding: 1.5em;
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    margin-bottom: 1.5em;
    transition: all 0.2s ease-in-out;
  }
  
  .professional-development > div:hover {
    transform: translateY(-3px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
  }
  
  .professional-development h3 {
    margin-top: 0;
    margin-bottom: 0.8em;
    color: var(--global-text-color);
    font-weight: 600;
    border-bottom: 1px solid var(--global-border-color);
    padding-bottom: 0.5em;
  }
  
  /* References */
  .references {
    padding: 1.5em;
    background-color: var(--global-bg-color);
    border: 1px solid var(--global-border-color);
    border-radius: 8px;
    text-align: center;
    font-style: italic;
    color: var(--global-text-color);
  }
  
  /* Icons */
  .fa, .fas, .far, .fab {
    color: var(--global-text-color);
  }
  
  .achievement-content .fas {
    color: inherit;
  }
  
  /* Responsive Adjustments */
  @media (max-width: 768px) {
    .research-interests,
    .skills-grid {
      grid-template-columns: 1fr;
    }
  }
</style>
